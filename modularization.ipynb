{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import index_files\n",
        "\n",
        "index_files('dataset/images', 'wb')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_images(path):\n",
        "  path = Path(path)\n",
        "  # Ensures only valid image files are loaded\n",
        "  img_paths = list(path.glob('*.jpg')) + list(path.glob('*.jpeg')) + list(path.glob('*.png')) \\\n",
        "              + list(path.glob('*.gif'))\n",
        "  images = []\n",
        "  filenames = []\n",
        "  print(f'Loading {len(img_paths)} images')\n",
        "  for img_path in tqdm(img_paths):\n",
        "    # load image\n",
        "    img = tf.keras.preprocessing.image.load_img(img_path, target_size=(224,224))\n",
        "    images.append(img)\n",
        "    filenames.append(img_path.name)\n",
        "  print('images loaded as' , type(images[0]), 'type')\n",
        "  return images, filenames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k6ipXouDSL1"
      },
      "outputs": [],
      "source": [
        "path = 'dataset/images/'\n",
        "images, filenames = load_images(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REea3mQpbp3-",
        "outputId": "6c8e72d0-4840-4c6e-c9d6-12da8535274a"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K \n",
        "\n",
        "model = tf.keras.applications.VGG19(\n",
        "    include_top=True,\n",
        "    weights=\"imagenet\",\n",
        "    input_tensor=None,\n",
        "    input_shape=None,\n",
        "    pooling=None,\n",
        "    classes=1000,\n",
        "    classifier_activation=\"softmax\",)\n",
        "\n",
        "model.summary()\n",
        "\n",
        "layer_outputs = [layer.output for layer in model.layers[1:-1]]  # Identifies layer outputs\n",
        "# Creates a model that will return the layer feature maps as outputs for a given image\n",
        "visual_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs) \n",
        "\n",
        "layer_names = [layer.name for layer in visual_model.layers]\n",
        "\n",
        "for i, layer in enumerate(layer_names):\n",
        "    print(i, layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJOYzfBS98c0"
      },
      "outputs": [],
      "source": [
        "def get_embeddings(f_levels, images, filenames=None):\n",
        "\n",
        "  # Create a single string with all the f_levels\n",
        "  levels = '_'.join([str(f) for f in f_levels])\n",
        "  path = Path(f'./dataset/embeddings/{levels}')\n",
        "  path.mkdir(parents=True, exist_ok=True)\n",
        "  # Initialize embeddings\n",
        "  E = []\n",
        "  for i, img in enumerate(images):\n",
        "    print('embedding image', i)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    img = tf.keras.applications.vgg19.preprocess_input(img)       # Pre-processes image for VGG19\n",
        "    # run an image through the network by making a prediction\n",
        "    feature_maps = visual_model.predict(img)\n",
        "    \n",
        "    temp = np.zeros((0))\n",
        "    for level in f_levels:\n",
        "      A = feature_maps[level]\n",
        "      # Flatten feature map to 2x2 matrix if it is 4D (e.g. Conv layer)\n",
        "      if len(A.shape) == 4:\n",
        "        A = A.reshape(A.shape[0], A.shape[1]*A.shape[2], A.shape[3], order='F')\n",
        "        A = A.reshape(A.shape[1], A.shape[2])\n",
        "        # Compute Gram matrix (cummulative co-activation of filter per layer)\n",
        "        G = np.matmul(np.transpose(A),A)\n",
        "        # Append this layer's flattened Gram matrix to images embedding\n",
        "        dummy = np.zeros(temp.shape[0] + G.flatten().shape[0])\n",
        "        dummy[:temp.shape[0]] = temp\n",
        "        dummy[temp.shape[0]:] = G.flatten()\n",
        "        emb = np.copy(dummy) \n",
        "      # Flatten feature map to 1D vector if it is 2D (e.g. FC layer)\n",
        "      else:\n",
        "        emb = np.copy(A.flatten())\n",
        "\n",
        "    E.append(np.copy(emb))\n",
        "\n",
        "    # convert layer names to a single string all of them concatenated\n",
        "    layers = '_'.join([str(f) for f in f_levels])\n",
        "\n",
        "    if filenames is not None:\n",
        "      # create dir to save embeddings if it doesn't exist\n",
        "      Path(f'./embeddings/{layers}').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "      # save embeddings as .npy files\n",
        "      filename = filenames[i].split('.')[0]\n",
        "      np.save(f'./dataset/embeddings/{layers}/{filename}', emb)\n",
        "\n",
        "    if i%10 == 0:\n",
        "      print(f'Getting embedding of img no. {i}, with shape {E[i].shape}, and {len(f_levels)} layers')\n",
        "    \n",
        "  return np.array(E)\n",
        "\n",
        "\n",
        "# Select the layer from which to extract feature maps (conv1_1 is layer 0)\n",
        "f_levels = [23]        # Choose from [1,4,9,14,19,22,23]\n",
        "\n",
        "# Get embeddings for 'test' dataset\n",
        "embeddings = get_embeddings(f_levels, images, filenames=filenames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "\n",
        "def remove_duplicates(dir, layer, thresh=0.3, delete=False):\n",
        "\n",
        "    num_duplicates = 0\n",
        "\n",
        "    # create duplicates folder from dir\n",
        "    Path(f'{dir}/duplicates/images').mkdir(parents=True, exist_ok=True)\n",
        "    Path(f'{dir}/duplicates/embeddings').mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Load embeddings\n",
        "    embeddings = []\n",
        "    img_filenames = []\n",
        "    emb_filenames = []\n",
        "    embedding_path = Path(dir) / 'embeddings' / layer\n",
        "    for file in embedding_path.glob('*.npy'):\n",
        "        embeddings.append(np.load(file))\n",
        "        emb_filenames.append(file.name)\n",
        "        img_filenames.append(file.name.split('.')[0] + '.jpg')\n",
        "    embeddings = np.array(embeddings)\n",
        "\n",
        "    print('embeddings shape', embeddings.shape)\n",
        "\n",
        "    # Fit 5 Nerest Neighbors model to embeddings\n",
        "    knnbr = NearestNeighbors(n_neighbors=5).fit(embeddings)\n",
        "\n",
        "    # Iterate through all files in embeddings dir\n",
        "    for i in range(embeddings.shape[0]):\n",
        "\n",
        "        print('Analyzing embedding and image', emb_filenames[i], img_filenames[i])\n",
        "        knn = knnbr.kneighbors(embeddings[i].reshape(1, -1))\n",
        "                    \n",
        "        # Gets the distance and indexes of the k nearest neighbors (minus the first one, which is the query image)\n",
        "        distances = knn[0][0][1:]\n",
        "        indexes = knn[1][0][1:]\n",
        "\n",
        "        # Remove duplicates\n",
        "        for j in range(len(indexes)):\n",
        "            index = indexes[j]\n",
        "            distance = distances[j]\n",
        "\n",
        "            if distance < thresh:\n",
        "                num_duplicates += 1\n",
        "                print('found duplicate', emb_filenames[i])\n",
        "                # Copy the duplicated image into the duplicates folder\n",
        "                image_path = Path(dir) / 'images' / img_filenames[i]\n",
        "                dup_image_path = Path(dir) / 'duplicates' / 'images' / f'{img_filenames[i]}_{distance}.jpg'\n",
        "                neighbor_image_path = Path(dir) / 'images' / img_filenames[index]\n",
        "                dup_neighbor_image_path = Path(dir) / 'duplicates' / 'images' / str(f'{img_filenames[i]}_dup.jpg')\n",
        "                # copy files to duplicates folder\n",
        "                shutil.copy(image_path, dup_image_path)\n",
        "                shutil.copy(neighbor_image_path, dup_neighbor_image_path)\n",
        "\n",
        "    print('Found a total of', num_duplicates/2, 'duplicates')\n",
        "                \n",
        "\n",
        "dir = 'dataset'\n",
        "remove_duplicates(dir, '23', thresh=30.0, delete=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyN+NrAdeZEl/yzMjv7uT6AQ",
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
